{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ds6TtnbfQlhP","executionInfo":{"status":"ok","timestamp":1682675254354,"user_tz":-330,"elapsed":30973,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"c8d3677a-a4b1-452c-987f-431065e619a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import imageio\n","import cv2\n","import os"],"metadata":{"id":"itJIrNvCYNdF","execution":{"iopub.status.busy":"2023-04-27T08:05:13.757759Z","iopub.execute_input":"2023-04-27T08:05:13.758689Z","iopub.status.idle":"2023-04-27T08:05:13.764608Z","shell.execute_reply.started":"2023-04-27T08:05:13.758647Z","shell.execute_reply":"2023-04-27T08:05:13.763555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from tensorflow.keras.utils import to_categorical\n","import itertools\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, LSTM, Dense,GRU"],"metadata":{"id":"CH9xscYLYNdJ","execution":{"iopub.status.busy":"2023-04-27T08:05:13.766387Z","iopub.execute_input":"2023-04-27T08:05:13.767166Z","iopub.status.idle":"2023-04-27T08:05:14.234847Z","shell.execute_reply.started":"2023-04-27T08:05:13.767124Z","shell.execute_reply":"2023-04-27T08:05:14.233803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras import callbacks\n","from keras import optimizers\n","from keras import Model\n","from keras.layers import Dropout, Flatten, Dense\n","from keras.optimizers import Adam\n","from keras.applications import VGG16,ResNet50,EfficientNetB7,InceptionV3,EfficientNetV2L\n","\n","from keras.utils import to_categorical\n","from keras.layers import TimeDistributed"],"metadata":{"id":"xOULPzuIYNdK","execution":{"iopub.status.busy":"2023-04-27T08:05:14.408365Z","iopub.execute_input":"2023-04-27T08:05:14.408712Z","iopub.status.idle":"2023-04-27T08:05:14.414847Z","shell.execute_reply.started":"2023-04-27T08:05:14.408672Z","shell.execute_reply":"2023-04-27T08:05:14.413628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##train"],"metadata":{"id":"Gw5RGVi3YNdL"}},{"cell_type":"code","source":["dataset_path = os.listdir('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21_Train')\n","\n","label_types = os.listdir('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21_Train')\n","print (label_types)"],"metadata":{"id":"feogYogpYNdN","outputId":"1f98496a-72d0-4b7f-c2c1-532d2f4a7d10","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-04-27T08:05:20.755034Z","iopub.execute_input":"2023-04-27T08:05:20.755722Z","iopub.status.idle":"2023-04-27T08:05:20.769634Z","shell.execute_reply.started":"2023-04-27T08:05:20.755677Z","shell.execute_reply":"2023-04-27T08:05:20.768341Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1682675269301,"user_tz":-330,"elapsed":848,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['WritingOnBoard', 'SoccerJuggling', 'SkateBoarding', 'SkyDiving', 'SoccerPenalty', 'Shotput', 'SumoWrestling', 'SalsaSpin', 'Rafting', 'WalkingWithDog', 'LongJump', 'IceDancing', 'PushUps', 'Playing Basketball', 'FloorGymnastics', 'JavelinThrow', 'FieldHockeyPenalty', 'Playing Cricket', 'PullUps', 'Playing musical instrument', 'ApplyEyeMakeup']\n"]}]},{"cell_type":"code","source":["rooms = []\n","\n","for item in dataset_path:\n"," # Get all the file names\n"," all_rooms = os.listdir('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21_Train' + '/' +item)\n","\n"," # Add them to the list\n"," for room in all_rooms:\n","    rooms.append((item, str('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21_Train' + '/' +item) + '/' + room))\n","\n","# Build a dataframe\n","train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_path'])\n","\n","# Import label encoder\n","from sklearn import preprocessing\n","\n","# label_encoder object knows how to understand word labels.\n","label_encoder = preprocessing.LabelEncoder()\n","\n","# Encode labels in column 'species'.\n","train_df['tag_lebel']= label_encoder.fit_transform(train_df['tag'])\n","print(train_df)"],"metadata":{"id":"TByzmlnxYNdQ","outputId":"2f53cd6c-76fe-4b8c-dd4a-bdabe7f26f9a","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-04-27T08:05:22.146471Z","iopub.execute_input":"2023-04-27T08:05:22.146828Z","iopub.status.idle":"2023-04-27T08:05:22.704229Z","shell.execute_reply.started":"2023-04-27T08:05:22.146795Z","shell.execute_reply":"2023-04-27T08:05:22.702967Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1682675277832,"user_tz":-330,"elapsed":8534,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                 tag                                         video_path  \\\n","0     WritingOnBoard  /content/drive/MyDrive/Deep_learning_project/D...   \n","1     WritingOnBoard  /content/drive/MyDrive/Deep_learning_project/D...   \n","2     WritingOnBoard  /content/drive/MyDrive/Deep_learning_project/D...   \n","3     WritingOnBoard  /content/drive/MyDrive/Deep_learning_project/D...   \n","4     WritingOnBoard  /content/drive/MyDrive/Deep_learning_project/D...   \n","...              ...                                                ...   \n","3072  ApplyEyeMakeup  /content/drive/MyDrive/Deep_learning_project/D...   \n","3073  ApplyEyeMakeup  /content/drive/MyDrive/Deep_learning_project/D...   \n","3074  ApplyEyeMakeup  /content/drive/MyDrive/Deep_learning_project/D...   \n","3075  ApplyEyeMakeup  /content/drive/MyDrive/Deep_learning_project/D...   \n","3076  ApplyEyeMakeup  /content/drive/MyDrive/Deep_learning_project/D...   \n","\n","      tag_lebel  \n","0            20  \n","1            20  \n","2            20  \n","3            20  \n","4            20  \n","...         ...  \n","3072          0  \n","3073          0  \n","3074          0  \n","3075          0  \n","3076          0  \n","\n","[3077 rows x 3 columns]\n"]}]},{"cell_type":"markdown","source":["#test"],"metadata":{"id":"xZkmKcWsYNdR"}},{"cell_type":"code","source":["dataset_path = os.listdir('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21 _Test')\n","\n","label_types = os.listdir('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21 _Test')\n","print (label_types)"],"metadata":{"id":"QYJTr5z4YNdR","outputId":"a303e11d-24b0-48fe-a319-b5dcb767629b","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-04-27T08:05:25.053573Z","iopub.execute_input":"2023-04-27T08:05:25.054768Z","iopub.status.idle":"2023-04-27T08:05:25.066372Z","shell.execute_reply.started":"2023-04-27T08:05:25.054712Z","shell.execute_reply":"2023-04-27T08:05:25.064968Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1682675277834,"user_tz":-330,"elapsed":14,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['SumoWrestling', 'SkyDiving', 'SalsaSpin', 'SoccerJuggling', 'WritingOnBoard', 'SkateBoarding', 'Shotput', 'SoccerPenalty', 'WalkingWithDog', 'IceDancing', 'PushUps', 'Playing musical instrument', 'Playing Basketball', 'LongJump', 'FloorGymnastics', 'JavelinThrow', 'Rafting', 'Playing Cricket', 'PullUps', 'ApplyEyeMakeup', 'FieldHockeyPenalty']\n"]}]},{"cell_type":"code","source":["rooms = []\n","\n","for item in dataset_path:\n"," # Get all the file names\n"," all_rooms = os.listdir('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21 _Test' + '/' +item)\n","\n"," # Add them to the list\n"," for room in all_rooms:\n","    rooms.append((item, str('/content/drive/MyDrive/Deep_learning_project/Dataset-UCF21/Dataset_UCF21 _Test' + '/' +item) + '/' + room))\n","\n","# Build a dataframe\n","test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_path'])\n","\n","# Import label encoder\n","from sklearn import preprocessing\n","\n","# label_encoder object knows how to understand word labels.\n","label_encoder = preprocessing.LabelEncoder()\n","\n","# Encode labels in column 'species'.\n","test_df['tag_lebel']= label_encoder.fit_transform(test_df['tag'])\n","print(test_df)"],"metadata":{"id":"iWLuG8nqYNdS","outputId":"3b6472d5-7325-459c-af35-882bc1fb4314","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-04-27T08:05:26.370717Z","iopub.execute_input":"2023-04-27T08:05:26.373165Z","iopub.status.idle":"2023-04-27T08:05:26.671003Z","shell.execute_reply.started":"2023-04-27T08:05:26.373122Z","shell.execute_reply":"2023-04-27T08:05:26.669803Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1682675281344,"user_tz":-330,"elapsed":3520,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                    tag                                         video_path  \\\n","0         SumoWrestling  /content/drive/MyDrive/Deep_learning_project/D...   \n","1         SumoWrestling  /content/drive/MyDrive/Deep_learning_project/D...   \n","2         SumoWrestling  /content/drive/MyDrive/Deep_learning_project/D...   \n","3         SumoWrestling  /content/drive/MyDrive/Deep_learning_project/D...   \n","4         SumoWrestling  /content/drive/MyDrive/Deep_learning_project/D...   \n","..                  ...                                                ...   \n","904  FieldHockeyPenalty  /content/drive/MyDrive/Deep_learning_project/D...   \n","905  FieldHockeyPenalty  /content/drive/MyDrive/Deep_learning_project/D...   \n","906  FieldHockeyPenalty  /content/drive/MyDrive/Deep_learning_project/D...   \n","907  FieldHockeyPenalty  /content/drive/MyDrive/Deep_learning_project/D...   \n","908  FieldHockeyPenalty  /content/drive/MyDrive/Deep_learning_project/D...   \n","\n","     tag_lebel  \n","0           18  \n","1           18  \n","2           18  \n","3           18  \n","4           18  \n","..         ...  \n","904          1  \n","905          1  \n","906          1  \n","907          1  \n","908          1  \n","\n","[909 rows x 3 columns]\n"]}]},{"cell_type":"code","source":["df = train_df.loc[: : ,['video_path','tag','tag_lebel']]\n","df\n","df.to_csv('train.csv', index=False)"],"metadata":{"id":"Ec0gZ698YNdT","execution":{"iopub.status.busy":"2023-04-27T08:05:38.082817Z","iopub.execute_input":"2023-04-27T08:05:38.083356Z","iopub.status.idle":"2023-04-27T08:05:38.118773Z","shell.execute_reply.started":"2023-04-27T08:05:38.083286Z","shell.execute_reply":"2023-04-27T08:05:38.116413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = test_df.loc[: : ,['video_path','tag','tag_lebel']]\n","df\n","df.to_csv('test.csv', index=False)"],"metadata":{"id":"SbRkGPuYYNdU","execution":{"iopub.status.busy":"2023-04-27T08:05:39.509268Z","iopub.execute_input":"2023-04-27T08:05:39.509971Z","iopub.status.idle":"2023-04-27T08:05:39.522496Z","shell.execute_reply.started":"2023-04-27T08:05:39.509928Z","shell.execute_reply":"2023-04-27T08:05:39.521128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(\"/content/drive/MyDrive/Deep_learning_project/train.csv\")\n","test_df = pd.read_csv(\"/content/drive/MyDrive/Deep_learning_project/test.csv\")\n","\n","print(f\"Total videos for training: {len(train_df)}\")\n","print(f\"Total videos for testing: {len(test_df)}\")"],"metadata":{"id":"PZwGA8eNYNdU","outputId":"b37c3852-d3b6-4914-8bc7-1c514e657d4b","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-04-27T08:06:07.167735Z","iopub.execute_input":"2023-04-27T08:06:07.168465Z","iopub.status.idle":"2023-04-27T08:06:07.187155Z","shell.execute_reply.started":"2023-04-27T08:06:07.168421Z","shell.execute_reply":"2023-04-27T08:06:07.185918Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1682675282069,"user_tz":-330,"elapsed":731,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total videos for training: 3077\n","Total videos for testing: 909\n"]}]},{"cell_type":"code","source":["train_path = train_df['video_path'].tolist()\n","train_label = train_df['tag_lebel'].tolist()\n","\n","test_path = test_df['video_path'].tolist()\n","test_label = test_df['tag_lebel'].tolist()"],"metadata":{"id":"EgRli4ShYNdV","execution":{"iopub.status.busy":"2023-04-27T08:06:11.600697Z","iopub.execute_input":"2023-04-27T08:06:11.601403Z","iopub.status.idle":"2023-04-27T08:06:11.608436Z","shell.execute_reply.started":"2023-04-27T08:06:11.601362Z","shell.execute_reply":"2023-04-27T08:06:11.607326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_path),len(train_label),len(test_path),len(test_label)"],"metadata":{"id":"THRNf-0cYNdW","outputId":"3eceb0c3-25cb-415a-a54c-395c6b05ad16","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-04-27T08:06:14.832650Z","iopub.execute_input":"2023-04-27T08:06:14.833569Z","iopub.status.idle":"2023-04-27T08:06:14.843706Z","shell.execute_reply.started":"2023-04-27T08:06:14.833515Z","shell.execute_reply":"2023-04-27T08:06:14.842561Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1682675284002,"user_tz":-330,"elapsed":7,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3077, 3077, 909, 909)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["\n","def crop_center_square(frame):\n","   y, x = frame.shape[0:2]\n","   min_dim = min(y, x)\n","   start_x = (x // 2) - (min_dim // 2)\n","   start_y = (y // 2) - (min_dim // 2)\n","   return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"],"metadata":{"id":"aompYJqWYNdW","execution":{"iopub.status.busy":"2023-04-27T08:06:17.240863Z","iopub.execute_input":"2023-04-27T08:06:17.241595Z","iopub.status.idle":"2023-04-27T08:06:17.247761Z","shell.execute_reply.started":"2023-04-27T08:06:17.241558Z","shell.execute_reply":"2023-04-27T08:06:17.246126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def frame_extraction(video_path, sequence_length=30):\n","    width=192\n","    height=192\n","    frames_list=[]\n","    #Read the Video\n","    video_reader = cv2.VideoCapture(video_path)                                                                      # open and access video sources\n","    #get the frame count\n","    frame_count=int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n","    #print(\"Frame count:\",frame_count)\n","    #Calculate the interval after which frames will be added to the list\n","    skip_interval = max(int(frame_count/(sequence_length-len(frames_list))), 1)\n","    #iterate through video frames\n","    for counter in range(sequence_length):\n","        #Set the current frame postion of the video\n","        video_reader.set(cv2.CAP_PROP_POS_FRAMES, counter * skip_interval)\n","        #Read the current frame\n","        ret, frame = video_reader.read()                                                                             #return(T,F),image(frame)=__.read()\n","        #print(cv2.imshow('frame', frame))\n","        if not ret:\n","            break;\n","        #Resize the image\n","        #frame = crop_center_square(frame)\n","        frame=cv2.resize(frame, (height, width))\n","\n","        #frame = frame/255\n","        #Append to the frame\n","#         plt.imshow(frame)\n","#         plt.show()\n","        frames_list.append(frame)\n","\n","    #Add padding frames if necessary\n","    if len(frames_list) < sequence_length:\n","        padding_frames = [np.zeros((height, width, 3), dtype=np.uint8) for _ in range(sequence_length - len(frames_list))]\n","        frames_list += padding_frames\n","\n","    video_reader.release()\n","    #Return the Frames List\n","    return frames_list"],"metadata":{"id":"TpNQoggJYNdX","execution":{"iopub.status.busy":"2023-04-27T08:06:20.583602Z","iopub.execute_input":"2023-04-27T08:06:20.584291Z","iopub.status.idle":"2023-04-27T08:06:20.592935Z","shell.execute_reply.started":"2023-04-27T08:06:20.584251Z","shell.execute_reply":"2023-04-27T08:06:20.591547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Function for Feature Extraction\n","# def feature_extraction(video_path):\n","#     width=224\n","#     height=224\n","#     sequence_length=20\n","#     frames_list=[]\n","#     #Read the Video\n","#     video_reader = cv2.VideoCapture(video_path)\n","#     #get the frame count\n","#     frame_count=int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n","#     #print(\"Frame count:\",frame_count)\n","#     #Calculate the interval after which frames will be added to the list\n","#     skip_interval = max(int(frame_count/sequence_length), 1)\n","#     #iterate through video frames\n","#     for counter in range(sequence_length):\n","#         #Set the current frame postion of the video\n","#         video_reader.set(cv2.CAP_PROP_POS_FRAMES, counter * skip_interval)\n","#         #Read the current frame\n","#         ret, frame = video_reader.read()\n","#         #print(cv2.imshow('frame', frame))\n","#         if not ret:\n","#             break;\n","#         #Resize the image\n","#         frame = crop_center_square(frame)\n","#         frame=cv2.resize(frame, (height, width))\n","\n","#         #frame = frame/255\n","#         #Append to the frame\n","# #         plt.imshow(frame)\n","# #         plt.show()\n","#         frames_list.append(frame)\n","#     video_reader.release()\n","#     #Return the Frames List\n","#     return frames_list"],"metadata":{"id":"AQ5Ck_7-YNdY","execution":{"iopub.status.busy":"2023-04-27T08:06:24.695425Z","iopub.execute_input":"2023-04-27T08:06:24.696339Z","iopub.status.idle":"2023-04-27T08:06:24.702334Z","shell.execute_reply.started":"2023-04-27T08:06:24.696282Z","shell.execute_reply":"2023-04-27T08:06:24.701135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Function for loading video files, Process and store in a data set\n","def load_videos(path):\n","    #global image\n","    #label_index=0\n","    labels=[]\n","    images=[]\n","    count=0\n","    for video_path in path:\n","        try:\n","            frames_list = frame_extraction(video_path)                                                # frames list of a vidio\n","            images.append(frames_list)\n","        except:\n","            print(len(frames_list),\"An error occured\")\n","\n","        if len(frames_list)!=30:\n","            print(len(frames_list),video_path)\n","            print(\"Error\")\n","            count=count+1\n","    #print(len(images))\n","    #print(count)\n","    return np.array(images,dtype='float32')                                                             # it give list of list (frames of vidios"],"metadata":{"id":"Q4aE6CkEYNdY","execution":{"iopub.status.busy":"2023-04-27T08:06:28.771625Z","iopub.execute_input":"2023-04-27T08:06:28.772879Z","iopub.status.idle":"2023-04-27T08:06:28.779986Z","shell.execute_reply.started":"2023-04-27T08:06:28.772826Z","shell.execute_reply":"2023-04-27T08:06:28.778870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################# '''CODE FOR DATA LOADER''' ################################\n","\n","from skimage.io import imread\n","from skimage.transform import resize\n","import numpy as np\n","import math\n","\n","# Here, `x_set` is list of path to the images\n","# `y_set` are the associated classes.\n","\n","# DEFINING CLASS FOR DATA LOADER IN BATCHES\n","class trainDataLoader(tf.keras.utils.Sequence):\n","\n","    def __init__(self, x_set, y_set, batch_size):\n","        self.x, self.y = x_set, y_set\n","        self.batch_size = batch_size\n","\n","    def __len__(self):\n","        return math.ceil(len(self.x) / self.batch_size)\n","\n","    def __getitem__(self, idx):\n","        batch_x = self.x[idx * self.batch_size:(idx + 1) *self.batch_size]\n","        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        #print(batch_y)\n","        batch_x_images = load_videos(batch_x)\n","\n","        return np.array(batch_x_images),np.array(batch_y)                                                  # it give list of list (frames of vidio of paticular batch) in array"],"metadata":{"id":"M1E-WFUoYNdZ","execution":{"iopub.status.busy":"2023-04-27T08:06:33.202392Z","iopub.execute_input":"2023-04-27T08:06:33.202760Z","iopub.status.idle":"2023-04-27T08:06:33.590987Z","shell.execute_reply.started":"2023-04-27T08:06:33.202726Z","shell.execute_reply":"2023-04-27T08:06:33.589942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ResNet50_Model = ResNet50(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n","#ResNet50_Model.summary()\n","for layer in ResNet50_Model.layers:\n","    layer.trainable = False\n","  #print('Layer ' + layer.name + ' frozen.')\n","\n","x =ResNet50_Model.layers[-33].output\n","flattenLayer = Flatten()(x)\n","# vggModel=Flatten(vggModel)\n","baseConv = Model(ResNet50_Model.input,flattenLayer)                                                       # take frame of 1 vidio and give feature vector for each frame\n","inputs = tf.keras.Input(shape=(30, 192, 192, 3))\n","temporalConvolution = TimeDistributed(baseConv)(inputs)                                                   # take a rame one by one pass through baseconv and get feature vector\n","# temporalConvolution.shape\n","#flattenLayer = Flatten(temporalConvolution)\n","x = LSTM(256)(temporalConvolution)                                                                        # provide all feature vector for one vidio in a lstm input\n","                                                                                                          # 256 is hidden layer size\n","#x = keras.layers.Dropout(0.1)(x)\n","#x = keras.layers.Dense(5000, activation=\"tanh\")(x)\n","x = keras.layers.Dense(500, activation=\"ReLU\")(x)\n","x = keras.layers.Dense(100, activation=\"ReLU\")(x)\n","output = keras.layers.Dense(21, activation=\"softmax\")(x)\n","\n","\n","model1 = Model(inputs, output)\n","model1.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model1.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-04-23T07:50:34.766598Z","iopub.execute_input":"2023-04-23T07:50:34.767536Z","iopub.status.idle":"2023-04-23T07:50:37.340918Z","shell.execute_reply.started":"2023-04-23T07:50:34.767494Z","shell.execute_reply":"2023-04-23T07:50:37.339822Z"},"id":"e-eJeFUAYNda","outputId":"c3a4519e-ee93-4a21-b8be-f62e3f6c7221","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"model_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_12 (InputLayer)       [(None, 30, 192, 192, 3)  0         \n                             ]                                   \n                                                                 \n time_distributed_5 (TimeDis  (None, 30, 147456)       8589184   \n tributed)                                                       \n                                                                 \n lstm_5 (LSTM)               (None, 256)               151258112 \n                                                                 \n dense_9 (Dense)             (None, 500)               128500    \n                                                                 \n dense_10 (Dense)            (None, 100)               50100     \n                                                                 \n dense_11 (Dense)            (None, 21)                2121      \n                                                                 \n=================================================================\nTotal params: 160,028,017\nTrainable params: 151,438,833\nNon-trainable params: 8,589,184\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history1 = model1.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-23T09:12:05.345571Z","iopub.execute_input":"2023-04-23T09:12:05.346218Z","iopub.status.idle":"2023-04-23T09:12:12.355266Z","shell.execute_reply.started":"2023-04-23T09:12:05.346178Z","shell.execute_reply":"2023-04-23T09:12:12.353165Z"},"id":"sOX7AnrrYNda","outputId":"1628dfe9-ace7-4048-f004-62e4881e7507","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1485793671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m history1 = model1.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     callbacks=[Es])\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[0;32m--> 135\u001b[0;31m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 53\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":["#model.save(\"resnet50.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-20T18:44:25.155395Z","iopub.execute_input":"2023-04-20T18:44:25.155790Z","iopub.status.idle":"2023-04-20T18:44:25.165165Z","shell.execute_reply.started":"2023-04-20T18:44:25.155754Z","shell.execute_reply":"2023-04-20T18:44:25.164015Z"},"id":"x6kKo3rfYNdb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EfficientNetV2L_Model = EfficientNetV2L(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n","#ResNet50_Model.summary()\n","for layer in EfficientNetV2L_Model.layers:\n","    layer.trainable = False\n","  #print('Layer ' + layer.name + ' frozen.')\n","\n","x =EfficientNetV2L_Model.layers[-1].output\n","flattenLayer = Flatten()(x)\n","# vggModel=Flatten(vggModel)\n","baseConv = Model(EfficientNetV2L_Model.input,flattenLayer)\n","inputs = tf.keras.Input(shape=(30,192,192, 3))\n","temporalConvolution = TimeDistributed(baseConv)(inputs)\n","# temporalConvolution.shape\n","#flattenLayer = Flatten(temporalConvolution)\n","#x = LSTM(256, return_sequences=True)(temporalConvolution)\n","x = LSTM(128)(temporalConvolution)\n","#x = keras.layers.Dropout(0.1)(x)\n","#x = keras.layers.Dense(5000, activation=\"tanh\")(x)\n","x = keras.layers.Dense(500, activation=\"ReLU\")(x)\n","x = keras.layers.Dense(100, activation=\"ReLU\")(x)\n","output = keras.layers.Dense(21, activation=\"softmax\")(x)\n","\n","\n","model2 = Model(inputs, output)\n","model2.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model2.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-04-24T09:44:53.377598Z","iopub.execute_input":"2023-04-24T09:44:53.378308Z","iopub.status.idle":"2023-04-24T09:45:18.474535Z","shell.execute_reply.started":"2023-04-24T09:44:53.378270Z","shell.execute_reply":"2023-04-24T09:45:18.473457Z"},"id":"MdRVcwpIYNdb","outputId":"b0367c06-47a7-4204-9263-6c5ba957be5a","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n473176280/473176280 [==============================] - 3s 0us/step\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 30, 192, 192, 3)  0         \n                             ]                                   \n                                                                 \n time_distributed (TimeDistr  (None, 30, 46080)        117746848 \n ibuted)                                                         \n                                                                 \n lstm (LSTM)                 (None, 128)               23659008  \n                                                                 \n dense (Dense)               (None, 500)               64500     \n                                                                 \n dense_1 (Dense)             (None, 100)               50100     \n                                                                 \n dense_2 (Dense)             (None, 21)                2121      \n                                                                 \n=================================================================\nTotal params: 141,522,577\nTrainable params: 23,775,729\nNon-trainable params: 117,746,848\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history2 = model2.fit(trainDataLoader(train_path,train_label,batch_size=8),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=8),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-24T09:45:49.278057Z","iopub.execute_input":"2023-04-24T09:45:49.278542Z","iopub.status.idle":"2023-04-24T12:30:28.854528Z","shell.execute_reply.started":"2023-04-24T09:45:49.278497Z","shell.execute_reply":"2023-04-24T12:30:28.853439Z"},"id":"KZJrCtnbYNdc","outputId":"e6d62881-5dd7-4570-9499-a7774ef8874a","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2023-04-24 09:46:12.979478: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/time_distributed/model/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"385/385 [==============================] - 2077s 5s/step - loss: 2.2591 - accuracy: 0.3442 - val_loss: 1.8250 - val_accuracy: 0.5303\nEpoch 2/5\n385/385 [==============================] - 1963s 5s/step - loss: 1.7443 - accuracy: 0.4768 - val_loss: 1.4799 - val_accuracy: 0.5941\nEpoch 3/5\n385/385 [==============================] - 1945s 5s/step - loss: 1.3864 - accuracy: 0.5609 - val_loss: 1.3372 - val_accuracy: 0.5325\nEpoch 4/5\n385/385 [==============================] - 1943s 5s/step - loss: 1.1022 - accuracy: 0.6360 - val_loss: 1.0914 - val_accuracy: 0.7052\nEpoch 5/5\n385/385 [==============================] - 1949s 5s/step - loss: 0.9701 - accuracy: 0.6828 - val_loss: 1.4613 - val_accuracy: 0.5655\n","output_type":"stream"}]},{"cell_type":"code","source":["model2.save(\"EfficientNetV2L.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-24T12:30:28.857194Z","iopub.execute_input":"2023-04-24T12:30:28.857551Z","iopub.status.idle":"2023-04-24T12:30:31.996190Z","shell.execute_reply.started":"2023-04-24T12:30:28.857513Z","shell.execute_reply":"2023-04-24T12:30:31.995106Z"},"id":"TW7SPCGUYNdd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history2 = model2.fit(trainDataLoader(train_path,train_label,batch_size=8),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=8),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-24T12:38:25.641390Z","iopub.execute_input":"2023-04-24T12:38:25.642031Z","iopub.status.idle":"2023-04-24T15:20:15.644596Z","shell.execute_reply.started":"2023-04-24T12:38:25.641991Z","shell.execute_reply":"2023-04-24T15:20:15.643332Z"},"id":"i3LaOHUDYNdd","outputId":"adb67f43-ccad-4c69-b305-586b8a6d6112","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n385/385 [==============================] - 1944s 5s/step - loss: 0.8709 - accuracy: 0.7186 - val_loss: 1.0477 - val_accuracy: 0.6799\nEpoch 2/5\n385/385 [==============================] - 1939s 5s/step - loss: 0.7763 - accuracy: 0.7485 - val_loss: 0.9830 - val_accuracy: 0.7030\nEpoch 3/5\n385/385 [==============================] - 1940s 5s/step - loss: 0.6801 - accuracy: 0.7748 - val_loss: 1.1079 - val_accuracy: 0.6799\nEpoch 4/5\n385/385 [==============================] - 1945s 5s/step - loss: 0.5927 - accuracy: 0.7992 - val_loss: 0.9374 - val_accuracy: 0.7338\nEpoch 5/5\n385/385 [==============================] - 1939s 5s/step - loss: 0.5624 - accuracy: 0.8170 - val_loss: 0.9374 - val_accuracy: 0.7360\n","output_type":"stream"}]},{"cell_type":"code","source":["model2.save(\"EfficientNetV2L-2.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-24T15:20:15.689151Z","iopub.execute_input":"2023-04-24T15:20:15.689883Z","iopub.status.idle":"2023-04-24T15:20:18.169742Z","shell.execute_reply.started":"2023-04-24T15:20:15.689830Z","shell.execute_reply":"2023-04-24T15:20:18.168658Z"},"id":"UfBWlnJEYNde","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Do not touch VGG16"],"metadata":{"id":"fFbxR7vxYNde"}},{"cell_type":"code","source":["VGG16_Model = VGG16(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n","#ResNet50_Model.summary()\n","for layer in VGG16_Model.layers:\n","    layer.trainable = False\n","  #print('Layer ' + layer.name + ' frozen.')\n","\n","x =VGG16_Model.layers[-1].output\n","flattenLayer = Flatten()(x)\n","# vggModel=Flatten(vggModel)\n","baseConv = Model(VGG16_Model.input,flattenLayer)\n","inputs = tf.keras.Input(shape=(30,192,192, 3))\n","temporalConvolution = TimeDistributed(baseConv)(inputs)\n","# temporalConvolution.shape\n","#flattenLayer = Flatten(temporalConvolution)\n","x = LSTM(256)(temporalConvolution)\n","\n","#x = keras.layers.Dropout(0.1)(x)\n","#x = keras.layers.Dense(5000, activation=\"tanh\")(x)\n","x = keras.layers.Dense(500, activation=\"ReLU\")(x)\n","x = keras.layers.Dense(100, activation=\"ReLU\")(x)\n","output = keras.layers.Dense(21, activation=\"softmax\")(x)\n","\n","\n","model3 = Model(inputs, output)\n","model3.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model3.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-04-22T04:10:57.518823Z","iopub.execute_input":"2023-04-22T04:10:57.519927Z","iopub.status.idle":"2023-04-22T04:11:03.372149Z","shell.execute_reply.started":"2023-04-22T04:10:57.519886Z","shell.execute_reply":"2023-04-22T04:11:03.371346Z"},"id":"lVSv16FWYNdf","outputId":"c18725fc-f700-4bdf-8956-b13492398124","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 30, 192, 192, 3)  0         \n                             ]                                   \n                                                                 \n time_distributed (TimeDistr  (None, 30, 18432)        14714688  \n ibuted)                                                         \n                                                                 \n lstm (LSTM)                 (None, 256)               19137536  \n                                                                 \n dense (Dense)               (None, 500)               128500    \n                                                                 \n dense_1 (Dense)             (None, 100)               50100     \n                                                                 \n dense_2 (Dense)             (None, 21)                2121      \n                                                                 \n=================================================================\nTotal params: 34,032,945\nTrainable params: 19,318,257\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history3 = model3.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-22T04:11:06.504675Z","iopub.execute_input":"2023-04-22T04:11:06.505497Z","iopub.status.idle":"2023-04-22T06:25:45.728174Z","shell.execute_reply.started":"2023-04-22T04:11:06.505462Z","shell.execute_reply":"2023-04-22T06:25:45.727160Z"},"id":"Fx_Kdk7HYNdg","outputId":"56b3b561-a1e1-4299-f763-38f72e93269e","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n308/308 [==============================] - 1733s 6s/step - loss: 2.3501 - accuracy: 0.3117 - val_loss: 1.9291 - val_accuracy: 0.4928\nEpoch 2/5\n308/308 [==============================] - 1587s 5s/step - loss: 1.6399 - accuracy: 0.4891 - val_loss: 1.4702 - val_accuracy: 0.5732\nEpoch 3/5\n308/308 [==============================] - 1587s 5s/step - loss: 1.2703 - accuracy: 0.5902 - val_loss: 1.4598 - val_accuracy: 0.5380\nEpoch 4/5\n308/308 [==============================] - 1587s 5s/step - loss: 0.9823 - accuracy: 0.6838 - val_loss: 1.0883 - val_accuracy: 0.6513\nEpoch 5/5\n308/308 [==============================] - 1582s 5s/step - loss: 0.8181 - accuracy: 0.7228 - val_loss: 1.0106 - val_accuracy: 0.7151\n","output_type":"stream"}]},{"cell_type":"code","source":["model3.save(\"VGG16.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:26:17.341497Z","iopub.execute_input":"2023-04-22T06:26:17.342578Z","iopub.status.idle":"2023-04-22T06:26:17.939924Z","shell.execute_reply.started":"2023-04-22T06:26:17.342536Z","shell.execute_reply":"2023-04-22T06:26:17.938859Z"},"id":"sFSl_8xkYNdg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history3 = model3.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-22T08:04:11.053674Z","iopub.execute_input":"2023-04-22T08:04:11.054149Z","iopub.status.idle":"2023-04-22T10:16:05.681795Z","shell.execute_reply.started":"2023-04-22T08:04:11.054105Z","shell.execute_reply":"2023-04-22T10:16:05.680703Z"},"id":"QrlOobl2YNdg","outputId":"0c408b53-b26b-4d7f-fb1b-bcb8cff4454e","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n308/308 [==============================] - 1605s 5s/step - loss: 0.6129 - accuracy: 0.8014 - val_loss: 1.0253 - val_accuracy: 0.6909\nEpoch 2/5\n308/308 [==============================] - 1579s 5s/step - loss: 0.5879 - accuracy: 0.7946 - val_loss: 0.9664 - val_accuracy: 0.7041\nEpoch 3/5\n308/308 [==============================] - 1577s 5s/step - loss: 0.5154 - accuracy: 0.8395 - val_loss: 0.9449 - val_accuracy: 0.7250\nEpoch 4/5\n308/308 [==============================] - 1568s 5s/step - loss: 0.4522 - accuracy: 0.8463 - val_loss: 0.9459 - val_accuracy: 0.7481\nEpoch 5/5\n308/308 [==============================] - 1575s 5s/step - loss: 0.3756 - accuracy: 0.8707 - val_loss: 0.8400 - val_accuracy: 0.7690\n","output_type":"stream"}]},{"cell_type":"code","source":["model3.save(\"VGG16-2.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-22T10:19:23.670307Z","iopub.execute_input":"2023-04-22T10:19:23.671339Z","iopub.status.idle":"2023-04-22T10:19:24.125612Z","shell.execute_reply.started":"2023-04-22T10:19:23.671291Z","shell.execute_reply":"2023-04-22T10:19:24.124548Z"},"id":"abe74AUiYNdh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model3.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-04-22T07:37:14.812839Z","iopub.execute_input":"2023-04-22T07:37:14.813573Z","iopub.status.idle":"2023-04-22T07:37:14.843531Z","shell.execute_reply.started":"2023-04-22T07:37:14.813534Z","shell.execute_reply":"2023-04-22T07:37:14.842526Z"},"id":"63riFruTYNdh","outputId":"8f6d5d0b-b101-49f7-aefc-c0a7eef13d5c","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 30, 192, 192, 3)  0         \n                             ]                                   \n                                                                 \n time_distributed (TimeDistr  (None, 30, 18432)        14714688  \n ibuted)                                                         \n                                                                 \n lstm (LSTM)                 (None, 256)               19137536  \n                                                                 \n dense (Dense)               (None, 500)               128500    \n                                                                 \n dense_1 (Dense)             (None, 100)               50100     \n                                                                 \n dense_2 (Dense)             (None, 21)                2121      \n                                                                 \n=================================================================\nTotal params: 34,032,945\nTrainable params: 19,318,257\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":[],"metadata":{"id":"IHqSGZt3YNdk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**ATTENTION**"],"metadata":{"id":"JS360NDNBpZx"}},{"cell_type":"code","source":["VGG16_Model = VGG16(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n","\n","for layer in VGG16_Model.layers:\n","    layer.trainable = False\n","\n","x = VGG16_Model.layers[-1].output\n","flattenLayer = Flatten()(x)\n","baseConv = Model(VGG16_Model.input, flattenLayer)\n","inputs = tf.keras.Input(shape=(30, 192, 192, 3))\n","temporalConvolution = TimeDistributed(baseConv)(inputs)\n","\n","# Apply first LSTM layer with return_state=True\n","x, state_h, state_c = LSTM(256, return_sequences=True, return_state=True)(temporalConvolution)               # return_sequences=True parameter indicates that the layer should return the full sequence of outputs for each input sequence(x will contain the sequence of LSTM outputs (if return_sequences=True), )\n","                                                                                                             # return_state=True parameter indicates that the layer should also return the hidden state and cell state at the last timestep.(state_h will contain the hidden state at the last timestep, and state_c will contain the cell state at the last timestep.)\n","# Apply Attention layer\n","attention = tf.keras.layers.Attention()([x, x])                                                              # attention apply on among output of 1st lstm\n","\n","# Concatenate attention output with the first LSTM layer output\n","concatenation = tf.keras.layers.Concatenate()([x, attention])                                                # attention vector concate with out of all inage vector\n","\n","# Apply second LSTM layer with the previous hidden and cell states\n","x, _, _ = LSTM(256, return_sequences=False, return_state=True)(concatenation, initial_state=[state_h, state_c])\n","\n","x = keras.layers.Dense(500, activation=\"ReLU\")(x)\n","x = keras.layers.Dense(100, activation=\"ReLU\")(x)\n","output = keras.layers.Dense(21, activation=\"softmax\")(x)\n","\n","model3 = Model(inputs, output)\n","model3.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n","model3.summary()"],"metadata":{"id":"ZWF6T6iQYNdk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fee937b1-69be-4f0e-b5ef-d83320e41e5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n58889256/58889256 [==============================] - 3s 0us/step\n\nModel: \"model_1\"\n\n__________________________________________________________________________________________________\n\n Layer (type)                   Output Shape         Param #     Connected to                     \n\n==================================================================================================\n\n input_2 (InputLayer)           [(None, 30, 192, 19  0           []                               \n\n                                2, 3)]                                                            \n\n                                                                                                  \n\n time_distributed (TimeDistribu  (None, 30, 18432)   14714688    ['input_2[0][0]']                \n\n ted)                                                                                             \n\n                                                                                                  \n\n lstm (LSTM)                    [(None, 30, 256),    19137536    ['time_distributed[0][0]']       \n\n                                 (None, 256),                                                     \n\n                                 (None, 256)]                                                     \n\n                                                                                                  \n\n attention (Attention)          (None, 30, 256)      0           ['lstm[0][0]',                   \n\n                                                                  'lstm[0][0]']                   \n\n                                                                                                  \n\n concatenate (Concatenate)      (None, 30, 512)      0           ['lstm[0][0]',                   \n\n                                                                  'attention[0][0]']              \n\n                                                                                                  \n\n lstm_1 (LSTM)                  [(None, 256),        787456      ['concatenate[0][0]',            \n\n                                 (None, 256),                     'lstm[0][1]',                   \n\n                                 (None, 256)]                     'lstm[0][2]']                   \n\n                                                                                                  \n\n dense (Dense)                  (None, 500)          128500      ['lstm_1[0][0]']                 \n\n                                                                                                  \n\n dense_1 (Dense)                (None, 100)          50100       ['dense[0][0]']                  \n\n                                                                                                  \n\n dense_2 (Dense)                (None, 21)           2121        ['dense_1[0][0]']                \n\n                                                                                                  \n\n==================================================================================================\n\nTotal params: 34,820,401\n\nTrainable params: 20,105,713\n\nNon-trainable params: 14,714,688\n\n__________________________________________________________________________________________________\n"}]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history3 = model3.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"id":"k4K51QlIYNdl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5fbed899-c5ec-4657-ce3f-134f8c785b83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1/5\n\n308/308 [==============================] - 4766s 15s/step - loss: 2.6103 - accuracy: 0.2720 - val_loss: 2.0292 - val_accuracy: 0.4609\n\nEpoch 2/5\n\n308/308 [==============================] - 1548s 5s/step - loss: 2.1490 - accuracy: 0.3432 - val_loss: 1.9276 - val_accuracy: 0.4609\n\nEpoch 3/5\n\n308/308 [==============================] - 1550s 5s/step - loss: 1.9327 - accuracy: 0.3763 - val_loss: 1.7148 - val_accuracy: 0.4906\n\nEpoch 4/5\n\n308/308 [==============================] - 1545s 5s/step - loss: 1.6592 - accuracy: 0.4556 - val_loss: 1.4834 - val_accuracy: 0.5545\n\nEpoch 5/5\n\n308/308 [==============================] - 1548s 5s/step - loss: 1.5163 - accuracy: 0.4771 - val_loss: 1.6101 - val_accuracy: 0.5655\n"}]},{"cell_type":"code","source":["model3.save(\"VGG16-2-att.h5\")"],"metadata":{"id":"uXIYZEOqYNdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VWFmTEjEQh9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VGG16_Model = VGG16(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n","#ResNet50_Model.summary()\n","for layer in VGG16_Model.layers:\n","    layer.trainable = False\n","  #print('Layer ' + layer.name + ' frozen.')\n","\n","x =VGG16_Model.layers[-1].output\n","flattenLayer = Flatten()(x)\n","# vggModel=Flatten(vggModel)\n","baseConv = Model(VGG16_Model.input,flattenLayer)\n","inputs = tf.keras.Input(shape=(30,192,192, 3))\n","temporalConvolution = TimeDistributed(baseConv)(inputs)\n","\n","query_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(temporalConvolution)\n","key_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(temporalConvolution)\n","\n","att_out = keras.layers.Attention()([query_img, key_img])\n","\n","concat_layer = keras.layers.Concatenate()([att_out, temporalConvolution])\n","\n","\n","\n","x = LSTM(256)(concat_layer)\n","\n","#x = keras.layers.Dropout(0.1)(x)\n","#x = keras.layers.Dense(5000, activation=\"tanh\")(x)\n","x = keras.layers.Dense(500, activation=\"ReLU\")(x)\n","x = keras.layers.Dense(100, activation=\"ReLU\")(x)\n","output = keras.layers.Dense(21, activation=\"softmax\")(x)\n","\n","\n","model4 = Model(inputs, output)\n","model4.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model4.summary()"],"metadata":{"id":"sFZGqOP9YNdm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8423139-0cda-491c-a091-c1377ccd9fe8","execution":{"iopub.status.busy":"2023-04-27T08:07:38.792560Z","iopub.execute_input":"2023-04-27T08:07:38.793344Z","iopub.status.idle":"2023-04-27T08:07:43.789788Z","shell.execute_reply.started":"2023-04-27T08:07:38.793304Z","shell.execute_reply":"2023-04-27T08:07:43.788958Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 30, 192, 19  0           []                               \n                                2, 3)]                                                            \n                                                                                                  \n time_distributed (TimeDistribu  (None, 30, 18432)   14714688    ['input_2[0][0]']                \n ted)                                                                                             \n                                                                                                  \n time_distributed_1 (TimeDistri  (None, 30, 128)     2359424     ['time_distributed[0][0]']       \n buted)                                                                                           \n                                                                                                  \n time_distributed_2 (TimeDistri  (None, 30, 128)     2359424     ['time_distributed[0][0]']       \n buted)                                                                                           \n                                                                                                  \n attention (Attention)          (None, 30, 128)      0           ['time_distributed_1[0][0]',     \n                                                                  'time_distributed_2[0][0]']     \n                                                                                                  \n concatenate (Concatenate)      (None, 30, 18560)    0           ['attention[0][0]',              \n                                                                  'time_distributed[0][0]']       \n                                                                                                  \n lstm (LSTM)                    (None, 256)          19268608    ['concatenate[0][0]']            \n                                                                                                  \n dense_2 (Dense)                (None, 500)          128500      ['lstm[0][0]']                   \n                                                                                                  \n dense_3 (Dense)                (None, 100)          50100       ['dense_2[0][0]']                \n                                                                                                  \n dense_4 (Dense)                (None, 21)           2121        ['dense_3[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 38,882,865\nTrainable params: 24,168,177\nNon-trainable params: 14,714,688\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history4 = model4.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAfXtjo1Ba4l","outputId":"7a4e3ca7-0287-4c18-d280-bc5a07d84e57","execution":{"iopub.status.busy":"2023-04-27T08:08:01.364715Z","iopub.execute_input":"2023-04-27T08:08:01.365531Z","iopub.status.idle":"2023-04-27T10:21:03.797031Z","shell.execute_reply.started":"2023-04-27T08:08:01.365487Z","shell.execute_reply":"2023-04-27T10:21:03.795888Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n308/308 [==============================] - 1721s 5s/step - loss: 2.4787 - accuracy: 0.2948 - val_loss: 2.1518 - val_accuracy: 0.4510\nEpoch 2/5\n308/308 [==============================] - 1559s 5s/step - loss: 1.9364 - accuracy: 0.4105 - val_loss: 1.8075 - val_accuracy: 0.4202\nEpoch 3/5\n308/308 [==============================] - 1562s 5s/step - loss: 1.5796 - accuracy: 0.4995 - val_loss: 1.3040 - val_accuracy: 0.6238\nEpoch 4/5\n308/308 [==============================] - 1556s 5s/step - loss: 1.3263 - accuracy: 0.5674 - val_loss: 1.2178 - val_accuracy: 0.6370\nEpoch 5/5\n308/308 [==============================] - 1550s 5s/step - loss: 1.1610 - accuracy: 0.6142 - val_loss: 1.2428 - val_accuracy: 0.6436\n","output_type":"stream"}]},{"cell_type":"code","source":["model4.save(\"VGG16-att.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:21:05.403163Z","iopub.execute_input":"2023-04-27T10:21:05.403775Z","iopub.status.idle":"2023-04-27T10:21:06.050831Z","shell.execute_reply.started":"2023-04-27T10:21:05.403730Z","shell.execute_reply":"2023-04-27T10:21:06.049794Z"},"trusted":true,"id":"hkar0YL7Qh9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history4 = model4.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:48:43.148340Z","iopub.execute_input":"2023-04-27T15:48:43.148910Z","iopub.status.idle":"2023-04-27T18:01:15.223523Z","shell.execute_reply.started":"2023-04-27T15:48:43.148840Z","shell.execute_reply":"2023-04-27T18:01:15.222509Z"},"trusted":true,"id":"vnp61owzQh9S","outputId":"b87fbae7-88b8-47fb-81e0-805dd1441dae"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n308/308 [==============================] - 1598s 5s/step - loss: 0.9883 - accuracy: 0.6542 - val_loss: 1.0566 - val_accuracy: 0.6689\nEpoch 2/5\n308/308 [==============================] - 1583s 5s/step - loss: 0.8568 - accuracy: 0.7114 - val_loss: 1.0198 - val_accuracy: 0.7030\nEpoch 3/5\n308/308 [==============================] - 1585s 5s/step - loss: 0.8986 - accuracy: 0.6916 - val_loss: 1.4722 - val_accuracy: 0.5116\nEpoch 4/5\n308/308 [==============================] - 1593s 5s/step - loss: 0.8060 - accuracy: 0.7205 - val_loss: 1.0142 - val_accuracy: 0.6832\nEpoch 5/5\n308/308 [==============================] - 1591s 5s/step - loss: 0.7320 - accuracy: 0.7498 - val_loss: 0.9098 - val_accuracy: 0.7316\n","output_type":"stream"}]},{"cell_type":"code","source":["model4.save(\"VGG16-2-att.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:08:29.859866Z","iopub.execute_input":"2023-04-27T18:08:29.860539Z","iopub.status.idle":"2023-04-27T18:08:30.664056Z","shell.execute_reply.started":"2023-04-27T18:08:29.860498Z","shell.execute_reply":"2023-04-27T18:08:30.662363Z"},"trusted":true,"id":"ySGMbo9DQh9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FRkwy8dTQh9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EfficientNetV2L_Model = EfficientNetV2L(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n","#ResNet50_Model.summary()\n","for layer in EfficientNetV2L_Model.layers:\n","    layer.trainable = False\n","  #print('Layer ' + layer.name + ' frozen.')\n","\n","x =EfficientNetV2L_Model.layers[-1].output\n","flattenLayer = Flatten()(x)\n","# vggModel=Flatten(vggModel)\n","baseConv = Model(EfficientNetV2L_Model.input,flattenLayer)\n","inputs = tf.keras.Input(shape=(30,192,192, 3))\n","temporalConvolution = TimeDistributed(baseConv)(inputs)\n","\n","query_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(temporalConvolution)\n","key_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(temporalConvolution)\n","\n","att_out = keras.layers.Attention()([query_img, key_img])\n","\n","concat_layer = keras.layers.Concatenate()([att_out, temporalConvolution])\n","\n","\n","\n","x = LSTM(256)(concat_layer)\n","\n","#x = keras.layers.Dropout(0.1)(x)\n","#x = keras.layers.Dense(5000, activation=\"tanh\")(x)\n","x = keras.layers.Dense(500, activation=\"ReLU\")(x)\n","x = keras.layers.Dense(100, activation=\"ReLU\")(x)\n","output = keras.layers.Dense(21, activation=\"softmax\")(x)\n","\n","\n","model5 = Model(inputs, output)\n","model5.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model5.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:21:06.052283Z","iopub.execute_input":"2023-04-27T10:21:06.052654Z","iopub.status.idle":"2023-04-27T10:21:26.111420Z","shell.execute_reply.started":"2023-04-27T10:21:06.052616Z","shell.execute_reply":"2023-04-27T10:21:26.110315Z"},"trusted":true,"id":"RFmMXHBFQh9T","outputId":"aeacf0bb-69e5-4bb5-bc8e-a0ac5d5204f8"},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n473176280/473176280 [==============================] - 3s 0us/step\nModel: \"model_3\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_4 (InputLayer)           [(None, 30, 192, 19  0           []                               \n                                2, 3)]                                                            \n                                                                                                  \n time_distributed_3 (TimeDistri  (None, 30, 46080)   117746848   ['input_4[0][0]']                \n buted)                                                                                           \n                                                                                                  \n time_distributed_4 (TimeDistri  (None, 30, 128)     5898368     ['time_distributed_3[0][0]']     \n buted)                                                                                           \n                                                                                                  \n time_distributed_5 (TimeDistri  (None, 30, 128)     5898368     ['time_distributed_3[0][0]']     \n buted)                                                                                           \n                                                                                                  \n attention_1 (Attention)        (None, 30, 128)      0           ['time_distributed_4[0][0]',     \n                                                                  'time_distributed_5[0][0]']     \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 30, 46208)    0           ['attention_1[0][0]',            \n                                                                  'time_distributed_3[0][0]']     \n                                                                                                  \n lstm_1 (LSTM)                  (None, 256)          47580160    ['concatenate_1[0][0]']          \n                                                                                                  \n dense_7 (Dense)                (None, 500)          128500      ['lstm_1[0][0]']                 \n                                                                                                  \n dense_8 (Dense)                (None, 100)          50100       ['dense_7[0][0]']                \n                                                                                                  \n dense_9 (Dense)                (None, 21)           2121        ['dense_8[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 177,304,465\nTrainable params: 59,557,617\nNon-trainable params: 117,746,848\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":["Es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history5 = model5.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:21:26.112956Z","iopub.execute_input":"2023-04-27T10:21:26.114062Z","iopub.status.idle":"2023-04-27T13:04:04.967226Z","shell.execute_reply.started":"2023-04-27T10:21:26.114021Z","shell.execute_reply":"2023-04-27T13:04:04.966086Z"},"trusted":true,"id":"sE3IzwYXQh9T","outputId":"4ddab093-c594-4bbd-95f8-d87d915eaf36"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2023-04-27 10:22:02.572621: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_3/time_distributed_3/model_2/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"308/308 [==============================] - 1992s 6s/step - loss: 2.3528 - accuracy: 0.3344 - val_loss: 1.9291 - val_accuracy: 0.5369\nEpoch 2/5\n308/308 [==============================] - 1944s 6s/step - loss: 1.9475 - accuracy: 0.4176 - val_loss: 1.9690 - val_accuracy: 0.4257\nEpoch 3/5\n308/308 [==============================] - 1944s 6s/step - loss: 1.7638 - accuracy: 0.4462 - val_loss: 1.5620 - val_accuracy: 0.5633\nEpoch 4/5\n308/308 [==============================] - 1934s 6s/step - loss: 1.5162 - accuracy: 0.5187 - val_loss: 1.4778 - val_accuracy: 0.5622\nEpoch 5/5\n308/308 [==============================] - 1938s 6s/step - loss: 1.2928 - accuracy: 0.5804 - val_loss: 1.2064 - val_accuracy: 0.6579\n","output_type":"stream"}]},{"cell_type":"code","source":["model5.save(\"EfficientNetV2L-att.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T13:04:04.969666Z","iopub.execute_input":"2023-04-27T13:04:04.970096Z","iopub.status.idle":"2023-04-27T13:04:09.246597Z","shell.execute_reply.started":"2023-04-27T13:04:04.970056Z","shell.execute_reply":"2023-04-27T13:04:09.244966Z"},"trusted":true,"id":"3qqUnmkJQh9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n","history5 = model5.fit(trainDataLoader(train_path,train_label,batch_size=10),epochs=5,\n","                    validation_data=trainDataLoader(test_path,test_label,batch_size=10),\n","                    callbacks=[Es])"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T13:04:09.250521Z","iopub.execute_input":"2023-04-27T13:04:09.251393Z","iopub.status.idle":"2023-04-27T15:47:25.788119Z","shell.execute_reply.started":"2023-04-27T13:04:09.251330Z","shell.execute_reply":"2023-04-27T15:47:25.786966Z"},"trusted":true,"id":"9wJoueHlQh9T","outputId":"543b6f06-dbc1-4e8c-a2bf-25a7c447de84"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n308/308 [==============================] - 1950s 6s/step - loss: 1.1219 - accuracy: 0.6412 - val_loss: 1.0943 - val_accuracy: 0.6799\nEpoch 2/5\n308/308 [==============================] - 1955s 6s/step - loss: 0.9834 - accuracy: 0.6890 - val_loss: 1.0510 - val_accuracy: 0.6832\nEpoch 3/5\n308/308 [==============================] - 1945s 6s/step - loss: 0.8784 - accuracy: 0.7095 - val_loss: 1.0671 - val_accuracy: 0.6964\nEpoch 4/5\n308/308 [==============================] - 1941s 6s/step - loss: 0.7716 - accuracy: 0.7413 - val_loss: 0.8896 - val_accuracy: 0.7272\nEpoch 5/5\n308/308 [==============================] - 1943s 6s/step - loss: 0.6778 - accuracy: 0.7780 - val_loss: 0.8117 - val_accuracy: 0.7536\n","output_type":"stream"}]},{"cell_type":"code","source":["model5.save(\"EfficientNetV2L-2-att.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:47:25.791459Z","iopub.execute_input":"2023-04-27T15:47:25.791870Z","iopub.status.idle":"2023-04-27T15:47:30.504840Z","shell.execute_reply.started":"2023-04-27T15:47:25.791830Z","shell.execute_reply":"2023-04-27T15:47:30.503146Z"},"trusted":true,"id":"0nqlRj4yQh9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["****Test the vidio random choosen****"],"metadata":{"id":"A3QxwP-hQh9U"}},{"cell_type":"code","source":["# Load the trained model\n","model = tf.keras.models.load_model('/content/drive/MyDrive/Deep_learning_project/EfficientNetV2L-2.h5')"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:09:54.888579Z","iopub.execute_input":"2023-04-27T18:09:54.889600Z","iopub.status.idle":"2023-04-27T18:10:20.122110Z","shell.execute_reply.started":"2023-04-27T18:09:54.889558Z","shell.execute_reply":"2023-04-27T18:10:20.120865Z"},"trusted":true,"id":"m9aRXYGSQh9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","Test_data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","# Select a random video path from the dataframe\n","test_video = random.choice(Test_data['video_path'])\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n"," # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","\n","\n","    # Get the corresponding tag from the dataframe\n","test_tag = data.loc[data['video_path'] == test_video, 'tag_lebel'].iloc[0]\n","pred_tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n","    # Print the predicted tag and its probability\n","print(f\"Test video tag: {test_tag}\")\n","#print(tag)\n","print(f\"Test video predicted tag: {class_index}, {pred_tag}\")\n","#print(f\"Test video predicted: {class_index}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")\n"],"metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:10:25.747447Z","iopub.execute_input":"2023-04-27T18:10:25.747816Z","iopub.status.idle":"2023-04-27T18:11:02.625697Z","shell.execute_reply.started":"2023-04-27T18:10:25.747781Z","shell.execute_reply":"2023-04-27T18:11:02.624607Z"},"trusted":true,"id":"tD9QQybzQh9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Test the  given vidio ***"],"metadata":{"id":"_zz8Uh4QQh9U"}},{"cell_type":"code","source":["# Load the trained model\n","model = tf.keras.models.load_model('/content/drive/MyDrive/Deep_learning_project/EfficientNetV2L-2-att.h5')"],"metadata":{"id":"HABAQkumQh9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test1.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhFsveOAQh9V","executionInfo":{"status":"ok","timestamp":1682671443793,"user_tz":-330,"elapsed":31034,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"2277e352-8cbf-4e1d-fead-e50643fa7433"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test1.mp4\n","1/1 [==============================] - 11s 11s/step\n","Test video predicted tag: 19, WalkingWithDog\n","  WalkingWithDog:  7.21%\n","  Shotput:  5.18%\n","  PullUps:  4.99%\n","  SumoWrestling:  4.90%\n","  IceDancing:  4.87%\n","  FloorGymnastics:  4.69%\n","  PushUps:  4.66%\n","  LongJump:  4.62%\n","  SoccerJuggling:  4.56%\n","  JavelinThrow:  4.55%\n","  Playing Cricket:  4.54%\n","  SkateBoarding:  4.54%\n","  SalsaSpin:  4.53%\n","  Rafting:  4.52%\n","  Playing Basketball:  4.52%\n","  SkyDiving:  4.52%\n","  WritingOnBoard:  4.52%\n","  FieldHockeyPenalty:  4.52%\n","  Playing musical instrument:  4.51%\n","  SoccerPenalty:  4.51%\n","  ApplyEyeMakeup:  4.51%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test2.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NWldmAC80xZ","executionInfo":{"status":"ok","timestamp":1682671448513,"user_tz":-330,"elapsed":2539,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"6610fc03-94a8-494f-cadb-9b135f0281cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test2.mp4\n","1/1 [==============================] - 0s 224ms/step\n","Test video predicted tag: 0, ApplyEyeMakeup\n","  ApplyEyeMakeup:  9.20%\n","  WritingOnBoard:  5.37%\n","  SkyDiving:  4.70%\n","  SkateBoarding:  4.60%\n","  WalkingWithDog:  4.55%\n","  Playing musical instrument:  4.48%\n","  PushUps:  4.47%\n","  LongJump:  4.47%\n","  Playing Basketball:  4.47%\n","  Playing Cricket:  4.47%\n","  PullUps:  4.47%\n","  Shotput:  4.47%\n","  JavelinThrow:  4.47%\n","  SalsaSpin:  4.47%\n","  FloorGymnastics:  4.47%\n","  SoccerPenalty:  4.47%\n","  Rafting:  4.47%\n","  SoccerJuggling:  4.47%\n","  IceDancing:  4.47%\n","  FieldHockeyPenalty:  4.47%\n","  SumoWrestling:  4.47%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test3.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLvknxgG814D","executionInfo":{"status":"ok","timestamp":1682671449871,"user_tz":-330,"elapsed":1406,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"c7498dba-ef35-42db-da08-9437c4d64a7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test3.mp4\n","1/1 [==============================] - 0s 179ms/step\n","Test video predicted tag: 0, ApplyEyeMakeup\n","  ApplyEyeMakeup: 11.97%\n","  PullUps:  4.40%\n","  FieldHockeyPenalty:  4.40%\n","  FloorGymnastics:  4.40%\n","  IceDancing:  4.40%\n","  JavelinThrow:  4.40%\n","  LongJump:  4.40%\n","  Playing Basketball:  4.40%\n","  Playing Cricket:  4.40%\n","  Playing musical instrument:  4.40%\n","  WritingOnBoard:  4.40%\n","  WalkingWithDog:  4.40%\n","  Rafting:  4.40%\n","  SalsaSpin:  4.40%\n","  Shotput:  4.40%\n","  SkateBoarding:  4.40%\n","  SkyDiving:  4.40%\n","  SoccerJuggling:  4.40%\n","  SoccerPenalty:  4.40%\n","  SumoWrestling:  4.40%\n","  PushUps:  4.40%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test4.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJTeR5LV87HH","executionInfo":{"status":"ok","timestamp":1682671458259,"user_tz":-330,"elapsed":8403,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"1fc6f8ad-bc74-48da-c010-ec2147648a31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test4.mp4\n","1/1 [==============================] - 0s 236ms/step\n","Test video predicted tag: 8, Playing musical instrument\n","  Playing musical instrument:  6.92%\n","  WalkingWithDog:  5.63%\n","  WritingOnBoard:  5.08%\n","  PushUps:  4.98%\n","  PullUps:  4.83%\n","  SkyDiving:  4.82%\n","  SkateBoarding:  4.52%\n","  SalsaSpin:  4.52%\n","  SumoWrestling:  4.52%\n","  ApplyEyeMakeup:  4.52%\n","  Rafting:  4.52%\n","  Shotput:  4.52%\n","  LongJump:  4.51%\n","  IceDancing:  4.51%\n","  FloorGymnastics:  4.51%\n","  Playing Cricket:  4.51%\n","  JavelinThrow:  4.51%\n","  Playing Basketball:  4.51%\n","  SoccerPenalty:  4.51%\n","  FieldHockeyPenalty:  4.51%\n","  SoccerJuggling:  4.51%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test5.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LJyqqFB88Pr","executionInfo":{"status":"ok","timestamp":1682671463941,"user_tz":-330,"elapsed":5733,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"79393730-b167-4252-9ded-8286ab8b5d65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test5.mp4\n","1/1 [==============================] - 0s 218ms/step\n","Test video predicted tag: 19, WalkingWithDog\n","  WalkingWithDog:  6.91%\n","  IceDancing:  5.64%\n","  PullUps:  4.93%\n","  FloorGymnastics:  4.78%\n","  Shotput:  4.71%\n","  SoccerJuggling:  4.69%\n","  LongJump:  4.65%\n","  SalsaSpin:  4.61%\n","  SkateBoarding:  4.61%\n","  SumoWrestling:  4.60%\n","  PushUps:  4.60%\n","  JavelinThrow:  4.56%\n","  Playing Cricket:  4.54%\n","  SkyDiving:  4.53%\n","  Playing Basketball:  4.52%\n","  WritingOnBoard:  4.52%\n","  Rafting:  4.52%\n","  FieldHockeyPenalty:  4.52%\n","  Playing musical instrument:  4.52%\n","  ApplyEyeMakeup:  4.52%\n","  SoccerPenalty:  4.52%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test6.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UtEMXRpY8853","executionInfo":{"status":"ok","timestamp":1682671469810,"user_tz":-330,"elapsed":5888,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"1c365901-d2e9-4e78-d726-44b19e322a3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test6.mp4\n","1/1 [==============================] - 0s 233ms/step\n","Test video predicted tag: 10, PushUps\n","  PushUps:  8.90%\n","  PullUps:  5.55%\n","  SumoWrestling:  4.66%\n","  WalkingWithDog:  4.60%\n","  Playing musical instrument:  4.53%\n","  Rafting:  4.51%\n","  IceDancing:  4.51%\n","  SalsaSpin:  4.49%\n","  SkyDiving:  4.49%\n","  WritingOnBoard:  4.48%\n","  SkateBoarding:  4.48%\n","  Shotput:  4.48%\n","  LongJump:  4.48%\n","  Playing Cricket:  4.48%\n","  FloorGymnastics:  4.48%\n","  SoccerJuggling:  4.48%\n","  JavelinThrow:  4.48%\n","  Playing Basketball:  4.48%\n","  ApplyEyeMakeup:  4.48%\n","  SoccerPenalty:  4.48%\n","  FieldHockeyPenalty:  4.48%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test7.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-UVqoKn89gH","executionInfo":{"status":"ok","timestamp":1682671473457,"user_tz":-330,"elapsed":3663,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"c497645d-9ff4-4b54-8893-e5a3e7688d17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test7.mp4\n","1/1 [==============================] - 0s 227ms/step\n","Test video predicted tag: 19, WalkingWithDog\n","  WalkingWithDog:  8.15%\n","  WritingOnBoard:  5.47%\n","  PullUps:  4.92%\n","  SkateBoarding:  4.72%\n","  PushUps:  4.65%\n","  SkyDiving:  4.64%\n","  Playing musical instrument:  4.50%\n","  SalsaSpin:  4.50%\n","  IceDancing:  4.50%\n","  ApplyEyeMakeup:  4.50%\n","  LongJump:  4.50%\n","  JavelinThrow:  4.50%\n","  SumoWrestling:  4.50%\n","  Playing Cricket:  4.50%\n","  FloorGymnastics:  4.50%\n","  Shotput:  4.50%\n","  SoccerJuggling:  4.50%\n","  Playing Basketball:  4.50%\n","  Rafting:  4.50%\n","  SoccerPenalty:  4.50%\n","  FieldHockeyPenalty:  4.50%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test8.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGNMuSmD8-ER","executionInfo":{"status":"ok","timestamp":1682671480838,"user_tz":-330,"elapsed":7386,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"ea047ba1-edb8-4a66-aaee-8e0ecbc70478"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test8.mp4\n","1/1 [==============================] - 0s 218ms/step\n","Test video predicted tag: 3, IceDancing\n","  IceDancing:  7.48%\n","  WalkingWithDog:  6.07%\n","  PullUps:  4.71%\n","  PushUps:  4.66%\n","  SoccerJuggling:  4.61%\n","  Shotput:  4.60%\n","  FloorGymnastics:  4.57%\n","  LongJump:  4.56%\n","  SumoWrestling:  4.55%\n","  SkateBoarding:  4.54%\n","  SalsaSpin:  4.53%\n","  Playing Cricket:  4.53%\n","  JavelinThrow:  4.52%\n","  SkyDiving:  4.51%\n","  Rafting:  4.51%\n","  Playing Basketball:  4.51%\n","  FieldHockeyPenalty:  4.50%\n","  WritingOnBoard:  4.50%\n","  Playing musical instrument:  4.50%\n","  SoccerPenalty:  4.50%\n","  ApplyEyeMakeup:  4.50%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test9.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQxfTX1U8-m1","executionInfo":{"status":"ok","timestamp":1682671486399,"user_tz":-330,"elapsed":5613,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"6c0410ff-0c4f-48fc-e98b-991e7feb6f93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test9.mp4\n","1/1 [==============================] - 0s 230ms/step\n","Test video predicted tag: 6, Playing Basketball\n","  Playing Basketball:  8.12%\n","  JavelinThrow:  5.52%\n","  FloorGymnastics:  4.80%\n","  FieldHockeyPenalty:  4.73%\n","  LongJump:  4.64%\n","  SoccerPenalty:  4.61%\n","  Shotput:  4.57%\n","  Playing Cricket:  4.56%\n","  SoccerJuggling:  4.51%\n","  SkateBoarding:  4.50%\n","  IceDancing:  4.50%\n","  WalkingWithDog:  4.50%\n","  SumoWrestling:  4.50%\n","  SalsaSpin:  4.50%\n","  SkyDiving:  4.50%\n","  PullUps:  4.50%\n","  Rafting:  4.50%\n","  WritingOnBoard:  4.50%\n","  PushUps:  4.50%\n","  ApplyEyeMakeup:  4.50%\n","  Playing musical instrument:  4.50%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","import cv2\n","import tensorflow as tf\n","\n","test_video=\"/content/drive/MyDrive/Deep_learning_project/test/test10.mp4\"\n","\n","# Define the input size of the model\n","IMG_SIZE = 192\n","\n","# Load the CSV file into a pandas dataframe\n","data = pd.read_csv('/content/drive/MyDrive/Deep_learning_project/test.csv')\n","\n","\n","\n","label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(data[\"tag\"]))\n","\n","\n","print(f\"Test video path: {test_video}\")\n","\n","\n","frames=frame_extraction(test_video, sequence_length=30)\n","x=np.array([frames])\n","x.shape\n","\n","\n","    # Apply the model to the frame\n","predictions = model.predict(x)\n","probability = tf.nn.softmax(predictions)[0]\n","class_index = tf.argmax(probability).numpy()\n","#class=class_index.numpy()\n","\n","    # Get the corresponding tag from the dataframe\n","tag = data.loc[data['tag_lebel'] == class_index, 'tag'].iloc[0]\n","\n"," # Print the predicted tag and its probability\n","#print(f\"Test video predicted : {class_index}\")\n","print(f\"Test video predicted tag: {class_index}, {tag}\")\n","\n","class_vocab = label_processor.get_vocabulary()\n","for i in np.argsort(probability)[::-1]:\n","    print(f\"  {class_vocab[i]}: {probability[i] * 100:5.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnHF45Ld-39z","executionInfo":{"status":"ok","timestamp":1682671489757,"user_tz":-330,"elapsed":3401,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"9d290851-42b8-4028-be56-a96fb4f5e4f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test video path: /content/drive/MyDrive/Deep_learning_project/test/test10.mp4\n","1/1 [==============================] - 0s 230ms/step\n","Test video predicted tag: 8, Playing musical instrument\n","  Playing musical instrument: 11.96%\n","  WritingOnBoard:  4.40%\n","  ApplyEyeMakeup:  4.40%\n","  SkyDiving:  4.40%\n","  PushUps:  4.40%\n","  PullUps:  4.40%\n","  WalkingWithDog:  4.40%\n","  SumoWrestling:  4.40%\n","  SalsaSpin:  4.40%\n","  Rafting:  4.40%\n","  Shotput:  4.40%\n","  SkateBoarding:  4.40%\n","  SoccerJuggling:  4.40%\n","  Playing Cricket:  4.40%\n","  Playing Basketball:  4.40%\n","  LongJump:  4.40%\n","  JavelinThrow:  4.40%\n","  IceDancing:  4.40%\n","  FloorGymnastics:  4.40%\n","  FieldHockeyPenalty:  4.40%\n","  SoccerPenalty:  4.40%\n"]}]}]}